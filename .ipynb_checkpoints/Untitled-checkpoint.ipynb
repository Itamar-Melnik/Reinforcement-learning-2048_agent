{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36f1a93-4dc7-444b-a6cb-8ad29a4dda9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_2048\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3dba50-f301-4dbd-81c9-c799d01939ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device set to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Preprocess the environment state\n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Converts one-hot encoded 2048 state (4,4,16) into flattened 1D vector with log2 values.\n",
    "\n",
    "    Args:\n",
    "        state (np.array): One-hot encoded (4, 4, 16) or raw (4, 4) grid.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Flattened tensor of shape (16,) with float values in [0, 11].\n",
    "    \"\"\"\n",
    "    if state.shape == (4, 4, 16):\n",
    "        # One-hot to index\n",
    "        indices = np.argmax(state, axis=2).astype(np.float32)\n",
    "    else:\n",
    "        # Convert values like 2, 4, 8 to log2\n",
    "        indices = np.zeros_like(state, dtype=np.float32)\n",
    "        non_zero = state > 0\n",
    "        indices[non_zero] = np.log2(state[non_zero])\n",
    "\n",
    "    flat = indices.flatten()  # (16,)\n",
    "    return torch.tensor(flat, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4d9b95-abac-42b3-9b0d-bf1be1a95408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact DQN for 2048 game.\n",
    "    Input: 16 cells (flattened 4x4 board with log2 values)\n",
    "    Output: 4 Q-values for each action (up, down, left, right)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size=16, hidden_size=128, action_size=4):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with log2 normalization.\n",
    "        Assumes input x is already a tensor of shape (batch_size, 16).\n",
    "        \"\"\"\n",
    "        # Clamp to avoid log(0), then normalize to [0,1]\n",
    "        x = torch.clamp(x, min=1e-5)\n",
    "        x = torch.log2(x) / 11.0  # log2(2048) = 11\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a867ed06-f2c9-4f2d-af45-580c49e2faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# Use CPU (since no GPU available)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network Agent with Double DQN, Experience Replay, and Soft Target Updates\n",
    "    Suitable for 2048 game or similar discrete environments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size=16, action_size=4, \n",
    "                 buffer_size=10000, batch_size=64, \n",
    "                 gamma=0.99, lr=0.001, tau=0.001, \n",
    "                 update_every=4):\n",
    "\n",
    "        # Environment dimensions\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma          # Discount factor\n",
    "        self.tau = tau              # Soft update interpolation factor\n",
    "        self.update_every = update_every\n",
    "\n",
    "        # Q-Networks\n",
    "        self.qnetwork_local = DQN(state_size, 128, action_size).to(device)\n",
    "        self.qnetwork_target = DQN(state_size, 128, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", \n",
    "                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "        # Step counter for timing updates\n",
    "        self.t_step = 0\n",
    "\n",
    "        # Epsilon-greedy parameters\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience and trigger learning periodically.\n",
    "        \"\"\"\n",
    "        self.memory.append(self.experience(state, action, reward, next_state, done))\n",
    "\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0 and len(self.memory) > self.batch_size:\n",
    "            experiences = self._sample_experiences()\n",
    "            self._learn(experiences)\n",
    "\n",
    "    def act(self, state, epsilon=None):\n",
    "        \"\"\"\n",
    "        Select action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        # Preprocess and prepare state tensor\n",
    "        processed_state = preprocess_state(state).unsqueeze(0)  # Shape: (1, 16)\n",
    "\n",
    "        # Predict Q-values (no gradients needed)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(processed_state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy choice\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def _sample_experiences(self):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch from replay buffer and preprocess it.\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = torch.stack([preprocess_state(e.state) for e in experiences]).to(device)\n",
    "        actions = torch.tensor([e.action for e in experiences], dtype=torch.long).unsqueeze(1).to(device)\n",
    "        rewards = torch.tensor([e.reward for e in experiences], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        next_states = torch.stack([preprocess_state(e.next_state) for e in experiences]).to(device)\n",
    "        dones = torch.tensor([e.done for e in experiences], dtype=torch.uint8).unsqueeze(1).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def _learn(self, experiences):\n",
    "        \"\"\"\n",
    "        Update Q-networks using Double DQN and soft target update.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Double DQN: get best next actions from local model\n",
    "        next_action_values = self.qnetwork_local(next_states).detach()\n",
    "        next_actions = next_action_values.max(1)[1].unsqueeze(1)\n",
    "\n",
    "        # Get next Q-values from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, next_actions)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Compute expected Q-values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Loss (Huber is more stable for big values like 2048)\n",
    "        loss = F.smooth_l1_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft update target network\n",
    "        self._soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def _soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        Soft update: θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save model parameters to file.\"\"\"\n",
    "        torch.save(self.qnetwork_local.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Load model parameters from file.\"\"\"\n",
    "        self.qnetwork_local.load_state_dict(torch.load(filename))\n",
    "        self.qnetwork_target.load_state_dict(torch.load(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31905697-76f0-4817-8774-5ba738329372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, avg_window=100, filename='scores.png', save=True):\n",
    "    \"\"\"\n",
    "    Plot the scores and their moving average.\n",
    "    \n",
    "    Args:\n",
    "        scores (list): List of episode scores.\n",
    "        avg_window (int): Window size for moving average.\n",
    "        filename (str): Filename to save the plot.\n",
    "        save (bool): Whether to save the plot as a file.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.arange(len(scores)), scores, label='Score')\n",
    "    \n",
    "    # Compute moving average\n",
    "    avg_scores = []\n",
    "    window = deque(maxlen=avg_window)\n",
    "    for score in scores:\n",
    "        window.append(score)\n",
    "        avg_scores.append(np.mean(window))\n",
    "    \n",
    "    plt.plot(np.arange(len(avg_scores)), avg_scores, 'r', label=f'Moving Avg ({avg_window})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Training Scores with Moving Average ({avg_window})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, env, n_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluate the agent's performance without exploration.\n",
    "    \n",
    "    Args:\n",
    "        agent: The trained DQN agent.\n",
    "        env: The 2048 environment.\n",
    "        n_episodes (int): Number of evaluation episodes.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation results including scores and max tiles.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    max_tiles = []\n",
    "    \n",
    "    for _ in tqdm(range(n_episodes), desc=\"Evaluating\"):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action = agent.act(state, epsilon=0.0)  # Pure greedy action\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "        \n",
    "        # Convert state to 4x4 grid and record max tile\n",
    "        processed_state = preprocess_state(state)\n",
    "        max_tile = np.max(processed_state)\n",
    "        \n",
    "        scores.append(score)\n",
    "        max_tiles.append(max_tile)\n",
    "    \n",
    "    return {\n",
    "        'avg_score': np.mean(scores),\n",
    "        'max_score': np.max(scores),\n",
    "        'avg_max_tile': np.mean(max_tiles),\n",
    "        'best_max_tile': np.max(max_tiles),\n",
    "        'all_scores': scores,\n",
    "        'all_max_tiles': max_tiles\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b969a4d-ac87-45ab-80e2-9b7a2df9f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2048(n_episodes=500, max_steps=10000, eval_freq=100):\n",
    "    \"\"\"\n",
    "    Train a DQN agent to play 2048\n",
    "    \n",
    "    Args:\n",
    "        n_episodes: Maximum number of training episodes\n",
    "        max_steps: Maximum number of steps per episode\n",
    "        eval_freq: Frequency of evaluation during training\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (scores, max_tiles) achieved during training\n",
    "    \"\"\"\n",
    "    env = gym.make('gymnasium_2048/TwentyFortyEight-v0')\n",
    "    \n",
    "    # Check state shape\n",
    "    state, _ = env.reset()\n",
    "    processed_state = preprocess_state(state)\n",
    "    print(f\"Original state shape: {state.shape}\")\n",
    "    print(f\"Processed state shape: {processed_state.shape}\")\n",
    "    print(f\"Sample processed state:\\n{processed_state}\")\n",
    "    \n",
    "    # Create agent with the correct parameters\n",
    "    agent = DQNAgent(state_size=16, action_size=4)  # 16 = 4x4 flattened\n",
    "    \n",
    "    # Score tracking\n",
    "    scores = []\n",
    "    max_tiles = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    # Create directory for saved models\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Tracking best performance\n",
    "    best_score = -np.inf\n",
    "    best_max_tile = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done or truncated)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        # Get the max tile value\n",
    "        processed_state = preprocess_state(state)\n",
    "        max_tile = np.max(processed_state)\n",
    "        \n",
    "        # Save results\n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        max_tiles.append(max_tile)\n",
    "        \n",
    "        # Print progress\n",
    "        if i_episode % 100 == 0:\n",
    "            end_time = time.time()\n",
    "            print(f'\\rEpisode {i_episode}\\tAvg Score: {np.mean(scores_window):.2f}\\tMax Tile: {max_tile}\\tEpsilon: {agent.epsilon:.2f}\\tTime: {end_time - start_time:.2f}s')\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # Save model if it's the best so far\n",
    "        if np.mean(scores_window) > best_score:\n",
    "            best_score = np.mean(scores_window)\n",
    "            agent.save('models/best_score_model.pth')\n",
    "        \n",
    "        if max_tile > best_max_tile:\n",
    "            best_max_tile = max_tile\n",
    "            agent.save('models/best_tile_model.pth')\n",
    "        \n",
    "        # Evaluate the model\n",
    "        if i_episode % eval_freq == 0:\n",
    "            results = evaluate_agent(agent, env)\n",
    "            print(f\"\\nEvaluation after {i_episode} episodes:\")\n",
    "            print(f\"Average Score: {results['avg_score']:.2f}\")\n",
    "            print(f\"Average Max Tile: {results['avg_max_tile']}\")\n",
    "            print(f\"Best Max Tile: {results['best_max_tile']}\")\n",
    "            print(\"\")\n",
    "            \n",
    "            # Save periodic model\n",
    "            agent.save(f'models/model_ep{i_episode}.pth')\n",
    "    \n",
    "    # Save final model\n",
    "    agent.save('models/final_model.pth')\n",
    "    \n",
    "    # Plot scores\n",
    "    plot_scores(scores)\n",
    "    \n",
    "    return scores, max_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf76e328-c339-4115-a97c-b6f44c1e12a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original state shape: (4, 4, 16)\n",
      "Processed state shape: (4, 4)\n",
      "Sample processed state:\n",
      "[[0 0 0 0]\n",
      " [0 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 2]]\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ec045c216a4c3793c68a0a1a8e9479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAvg Score: 739.52\tMax Tile: 64\tEpsilon: 0.01\tTime: 337.30s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the agent for 500 episodes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m scores, max_tiles \u001b[38;5;241m=\u001b[39m train_2048(n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest max tile achieved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmax(max_tiles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 80\u001b[0m, in \u001b[0;36mtrain_2048\u001b[1;34m(n_episodes, max_steps, eval_freq)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i_episode \u001b[38;5;241m%\u001b[39m eval_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 80\u001b[0m     results \u001b[38;5;241m=\u001b[39m evaluate_agent(agent, env)\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_episode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m episodes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[1;34m(agent, env, n_episodes)\u001b[0m\n\u001b[0;32m     42\u001b[0m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (done \u001b[38;5;129;01mor\u001b[39;00m truncated):\n\u001b[1;32m---> 45\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)  \u001b[38;5;66;03m# No exploration during evaluation\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     47\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[4], line 64\u001b[0m, in \u001b[0;36mDQNAgent.act\u001b[1;34m(self, state, epsilon)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 64\u001b[0m     action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local(state_tensor)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Epsilon-greedy action selection\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog2(torch\u001b[38;5;241m.\u001b[39mmaximum(x, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m))) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m11.0\u001b[39m  \u001b[38;5;66;03m# log2(2048) = 11\u001b[39;00m\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the agent for 500 episodes\n",
    "scores, max_tiles = train_2048(n_episodes=500, eval_freq=100)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best max tile achieved: {np.max(max_tiles)}\")\n",
    "print(f\"Average score over last 100 episodes: {np.mean(scores[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b8ea3-4afc-4664-b7d9-48c14f5de665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a comprehensive evaluation\n",
    "def evaluate_trained_agent(model_path='models/final_model.pth', n_episodes=10):\n",
    "    \"\"\"Run a comprehensive evaluation of the trained agent\"\"\"\n",
    "    env = gym.make('2048-v0')\n",
    "    agent = DQNAgent(state_size=16, action_size=4)\n",
    "    \n",
    "    # Load the model\n",
    "    agent.load(model_path)\n",
    "    \n",
    "    results = evaluate_agent(agent, env, n_episodes=n_episodes)\n",
    "    \n",
    "    print(f\"Results after {n_episodes} games:\")\n",
    "    print(f\"Average Score: {results['avg_score']:.2f}\")\n",
    "    print(f\"Maximum Score: {results['max_score']:.2f}\")\n",
    "    print(f\"Average Max Tile: {results['avg_max_tile']}\")\n",
    "    print(f\"Best Max Tile: {results['best_max_tile']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bbe75-67eb-4afb-9f4d-2676d5966feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the agent play a game (if render_mode is supported)\n",
    "try:\n",
    "    max_tile = play_2048_visual(model_path='models/best_tile_model.pth', n_episodes=1)\n",
    "    print(f\"Highest tile achieved: {max_tile}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize the game: {e}\")\n",
    "    print(\"This might be due to render mode not being supported in your environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aee6f0-5434-47f9-863d-3a60d965741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_2048_visual(model_path='models/final_model.pth', n_episodes=1):\n",
    "    \"\"\"\n",
    "    Play 2048 with a trained model and visualize the game\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the trained model\n",
    "        n_episodes: Number of episodes to play\n",
    "        \n",
    "    Returns:\n",
    "        int: Highest tile achieved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        env = gym.make('gymnasium_2048/TwentyFortyEight-v0', render_mode='human')\n",
    "    except:\n",
    "        env = gym.make('gymnasium_2048/TwentyFortyEight-v0')  # Fall back if visualization is not available\n",
    "        \n",
    "    agent = DQNAgent(state_size=16, action_size=4)\n",
    "    \n",
    "    # Load the model\n",
    "    agent.load(model_path)\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        try:\n",
    "            env.render()  # Show initial state\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action = agent.act(state, epsilon=0.0)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            try:\n",
    "                env.render()\n",
    "                time.sleep(0.3)  # Delay so we can see the moves\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Process final state to get max tile\n",
    "        processed_state = preprocess_state(state)\n",
    "        max_tile = np.max(processed_state)\n",
    "        \n",
    "        print(f\"Game {ep+1} finished. Score: {total_reward}, Max Tile: {max_tile}\")\n",
    "    \n",
    "    return max_tile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
