{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903ef56-6ed7-4fb1-84bd-ef796a9ad534",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdkfksj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43031fc-8406-42a8-b067-ddef98be25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import traci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad868a3-8130-43a2-b5d9-802ff4b0e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# הגדר נתיב ל-SUMO\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    # הגדר את הנתיב ישירות אם הוא לא מוגדר כמשתנה סביבה\n",
    "    sumo_path = r\"C:\\Program Files (x86)\\Eclipse\\Sumo\"  # שנה לנתיב שלך\n",
    "    os.environ['SUMO_HOME'] = sumo_path\n",
    "    tools = os.path.join(sumo_path, 'tools')\n",
    "    sys.path.append(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba31f28-2bf1-4e23-b4fc-240b51e92927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSimEnv:\n",
    "    \"\"\"SUMO traffic simulation environment for RL agents.\"\"\"\n",
    "    \n",
    "    def __init__(self, sumo_config, max_steps=3600):\n",
    "        \"\"\"\n",
    "        Initialize the environment.\n",
    "        \n",
    "        Args:\n",
    "            sumo_config: Path to SUMO configuration file\n",
    "            max_steps: Maximum number of steps per episode\n",
    "        \"\"\"\n",
    "        self.sumo_config = sumo_config\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Traffic light IDs\n",
    "        self.traffic_lights = [\"A\", \"B\"]\n",
    "        \n",
    "        # State and action space dimensions\n",
    "        self.state_size = 30  # Will be defined by _get_state shape\n",
    "        self.action_size = 4  # 4 possible green phases per intersection\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment and start a new episode.\"\"\"\n",
    "        # Close previous simulation if still running\n",
    "        if hasattr(self, 'sumo_running') and self.sumo_running:\n",
    "            traci.close()\n",
    "        \n",
    "        # Start SUMO simulation\n",
    "        if 'SUMO_HOME' in os.environ:\n",
    "            sumo_binary = os.path.join(os.environ['SUMO_HOME'], 'bin', 'sumo')\n",
    "        else:\n",
    "            sumo_binary = \"sumo\"\n",
    "            \n",
    "        sumo_cmd = [sumo_binary, \"-c\", self.sumo_config, \"--no-step-log\", \"true\", \"--no-warnings\", \"true\"]\n",
    "        traci.start(sumo_cmd)\n",
    "        self.sumo_running = True\n",
    "        \n",
    "        self.current_step = 0\n",
    "        \n",
    "        return self._get_state()\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment.\n",
    "        \n",
    "        Args:\n",
    "            action: List of actions [action_A, action_B] for each traffic light\n",
    "            \n",
    "        Returns:\n",
    "            next_state: New state after action\n",
    "            reward: Reward for the action\n",
    "            done: Whether the episode is finished\n",
    "            info: Additional information (empty dict for now)\n",
    "        \"\"\"\n",
    "        # Apply the action in the simulation\n",
    "        self._apply_action(action)\n",
    "        \n",
    "        # Advance simulation one step\n",
    "        traci.simulationStep()\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Get new state and calculate reward\n",
    "        next_state = self._get_state()\n",
    "        reward = self._calculate_reward()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (self.current_step >= self.max_steps or \n",
    "                traci.simulation.getMinExpectedNumber() <= 0)\n",
    "        \n",
    "        return next_state, reward, done, {}\n",
    "        \n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Get the current state of the environment.\n",
    "        \n",
    "        Returns:\n",
    "            state: Array representing the current state\n",
    "        \"\"\"\n",
    "        state = []\n",
    "        \n",
    "        # For each traffic light (A and B)\n",
    "        for tl_id in self.traffic_lights:\n",
    "            # Get incoming lanes for the traffic light\n",
    "            incoming_lanes = self._get_incoming_lanes(tl_id)\n",
    "            \n",
    "            for lane in incoming_lanes:\n",
    "                # Queue length in each lane\n",
    "                queue_length = traci.lane.getLastStepHaltingNumber(lane)\n",
    "                state.append(queue_length)\n",
    "                \n",
    "                # Average waiting time in each lane\n",
    "                waiting_time = traci.lane.getWaitingTime(lane)\n",
    "                state.append(waiting_time)\n",
    "                \n",
    "                # Number of approaching vehicles (not stopped yet)\n",
    "                vehicles = traci.lane.getLastStepVehicleIDs(lane)\n",
    "                approaching_vehicles = 0\n",
    "                for veh in vehicles:\n",
    "                    if not traci.vehicle.isStopped(veh):\n",
    "                        approaching_vehicles += 1\n",
    "                state.append(approaching_vehicles)\n",
    "            \n",
    "            # Current traffic light phase\n",
    "            current_phase = traci.trafficlight.getPhase(tl_id)\n",
    "            # Normalize to 0,1,2,3 (because we have 4 green phases)\n",
    "            normalized_phase = current_phase // 2\n",
    "            state.append(normalized_phase)\n",
    "            \n",
    "            # Time elapsed since last phase change\n",
    "            phase_duration = traci.trafficlight.getPhaseDuration(tl_id) - traci.trafficlight.getNextSwitch(tl_id)\n",
    "            state.append(phase_duration)\n",
    "        \n",
    "        return np.array(state)\n",
    "    \n",
    "    def _get_incoming_lanes(self, tl_id):\n",
    "        \"\"\"\n",
    "        Get all incoming lanes for a traffic light.\n",
    "        \n",
    "        Args:\n",
    "            tl_id: Traffic light ID\n",
    "            \n",
    "        Returns:\n",
    "            List of lane IDs\n",
    "        \"\"\"\n",
    "        links = traci.trafficlight.getControlledLinks(tl_id)\n",
    "        incoming_lanes = []\n",
    "        \n",
    "        for link in links:\n",
    "            if link:  # Some links might be empty\n",
    "                incoming_lane = link[0][0]  # Format is ((in_lane, out_lane, via_lane), ...)\n",
    "                if incoming_lane not in incoming_lanes:\n",
    "                    incoming_lanes.append(incoming_lane)\n",
    "        \n",
    "        return incoming_lanes\n",
    "    \n",
    "    def _apply_action(self, action):\n",
    "        \"\"\"\n",
    "        Apply the selected action to the environment.\n",
    "        \n",
    "        Args:\n",
    "            action: List of actions [action_A, action_B] for each traffic light\n",
    "        \"\"\"\n",
    "        # For each traffic light\n",
    "        for i, tl_id in enumerate(self.traffic_lights):\n",
    "            # Convert action to target phase (0,1,2,3 -> 0,2,4,6 which are green phases)\n",
    "            target_phase = action[i] * 2\n",
    "            current_phase = traci.trafficlight.getPhase(tl_id)\n",
    "            \n",
    "            # If the target phase is different from the current phase\n",
    "            if current_phase != target_phase:\n",
    "                # If we're in a green phase, we need to go through yellow first\n",
    "                if current_phase % 2 == 0:  # Phases 0,2,4,6 are green\n",
    "                    yellow_phase = current_phase + 1\n",
    "                    traci.trafficlight.setPhase(tl_id, yellow_phase)\n",
    "                    \n",
    "                    # Wait 3 seconds (yellow phase)\n",
    "                    for _ in range(3):\n",
    "                        traci.simulationStep()\n",
    "                \n",
    "                # Now go to the target green phase\n",
    "                traci.trafficlight.setPhase(tl_id, target_phase)\n",
    "    \n",
    "    def _calculate_reward(self):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on waiting time and queue length.\n",
    "        \n",
    "        Returns:\n",
    "            reward: Calculated reward value\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        \n",
    "        # Calculate total waiting time and number of vehicles\n",
    "        total_waiting_time = 0\n",
    "        total_vehicles = 0\n",
    "        \n",
    "        for tl_id in self.traffic_lights:\n",
    "            incoming_lanes = self._get_incoming_lanes(tl_id)\n",
    "            \n",
    "            for lane in incoming_lanes:\n",
    "                waiting_time = traci.lane.getWaitingTime(lane)\n",
    "                total_waiting_time += waiting_time\n",
    "                \n",
    "                halting_vehicles = traci.lane.getLastStepHaltingNumber(lane)\n",
    "                total_vehicles += halting_vehicles\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if total_vehicles > 0:\n",
    "            avg_waiting_time = total_waiting_time / total_vehicles\n",
    "        else:\n",
    "            avg_waiting_time = 0\n",
    "        \n",
    "        # Reward is negative of waiting time and vehicle count\n",
    "        waiting_time_penalty = -avg_waiting_time\n",
    "        vehicles_penalty = -total_vehicles * 0.1\n",
    "        \n",
    "        reward = waiting_time_penalty + vehicles_penalty\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the SUMO simulation.\"\"\"\n",
    "        if hasattr(self, 'sumo_running') and self.sumo_running:\n",
    "            traci.close()\n",
    "            self.sumo_running = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ecf298-871a-48f6-8742-4db58e03de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network Agent for traffic signal control.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent.\n",
    "        \n",
    "        Args:\n",
    "            state_size: Dimension of state space\n",
    "            action_size: Dimension of action space per intersection\n",
    "            learning_rate: Learning rate for the neural network\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # Discount factor\n",
    "        self.epsilon = 1.0   # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build separate models for each intersection\n",
    "        self.model_A = self._build_model()\n",
    "        self.model_B = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build a neural network for Q-function approximation.\n",
    "        \n",
    "        Returns:\n",
    "            Compiled Keras model\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        # שורה זו היא הבעייתית - משתמשת ב-lr במקום learning_rate\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience in replay memory.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            done: Whether the episode is done\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Choose action according to epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            \n",
    "        Returns:\n",
    "            Selected actions for each intersection [action_A, action_B]\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return [np.random.randint(self.action_size), \n",
    "                    np.random.randint(self.action_size)]\n",
    "        \n",
    "        # Exploitation: predict Q-values and choose best action\n",
    "        act_values_A = self.model_A.predict(state.reshape(1, -1))\n",
    "        act_values_B = self.model_B.predict(state.reshape(1, -1))\n",
    "        \n",
    "        return [np.argmax(act_values_A[0]), np.argmax(act_values_B[0])]\n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"\n",
    "        Train the agent with experiences from replay memory.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of samples to use for training\n",
    "        \"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "            \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Predict Q-values for next state\n",
    "            next_act_values_A = self.model_A.predict(next_state.reshape(1, -1))\n",
    "            next_act_values_B = self.model_B.predict(next_state.reshape(1, -1))\n",
    "            \n",
    "            # Calculate target Q-value for each intersection\n",
    "            target_A = reward\n",
    "            target_B = reward\n",
    "            \n",
    "            if not done:\n",
    "                target_A += self.gamma * np.amax(next_act_values_A[0])\n",
    "                target_B += self.gamma * np.amax(next_act_values_B[0])\n",
    "            \n",
    "            # Update Q-values in the model\n",
    "            target_f_A = self.model_A.predict(state.reshape(1, -1))\n",
    "            target_f_B = self.model_B.predict(state.reshape(1, -1))\n",
    "            \n",
    "            target_f_A[0][action[0]] = target_A\n",
    "            target_f_B[0][action[1]] = target_B\n",
    "            \n",
    "            # Train the models\n",
    "            self.model_A.fit(state.reshape(1, -1), target_f_A, epochs=1, verbose=0)\n",
    "            self.model_B.fit(state.reshape(1, -1), target_f_B, epochs=1, verbose=0)\n",
    "            \n",
    "        # Update exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def save(self, directory=\"models\"):\n",
    "        \"\"\"\n",
    "        Save the agent models.\n",
    "        \n",
    "        Args:\n",
    "            directory: Directory to save models in\n",
    "        \"\"\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        self.model_A.save(os.path.join(directory, \"model_A.h5\"))\n",
    "        self.model_B.save(os.path.join(directory, \"model_B.h5\"))\n",
    "        \n",
    "    def load(self, directory=\"models\"):\n",
    "        \"\"\"\n",
    "        Load the agent models.\n",
    "        \n",
    "        Args:\n",
    "            directory: Directory to load models from\n",
    "        \"\"\"\n",
    "        self.model_A = tf.keras.models.load_model(os.path.join(directory, \"model_A.h5\"))\n",
    "        self.model_B = tf.keras.models.load_model(os.path.join(directory, \"model_B.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb3f800-90a2-48b6-8dee-f0595db813b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(sumo_config, episodes=100, batch_size=32, model_dir=\"models\", stats_dir=\"stats\"):\n",
    "    \"\"\"\n",
    "    Train the RL agent.\n",
    "    \n",
    "    Args:\n",
    "        sumo_config: Path to SUMO configuration file\n",
    "        episodes: Number of episodes to train for\n",
    "        batch_size: Batch size for training\n",
    "        model_dir: Directory to save models\n",
    "        stats_dir: Directory to save training statistics\n",
    "    \n",
    "    Returns:\n",
    "        Trained agent\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    if not os.path.exists(stats_dir):\n",
    "        os.makedirs(stats_dir)\n",
    "    \n",
    "    # Initialize environment and agent\n",
    "    env = TrafficSimEnv(sumo_config)\n",
    "    agent = DQNAgent(state_size=env.state_size, action_size=env.action_size)\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        'episode_rewards': [],\n",
    "        'average_waiting_times': [],\n",
    "        'average_vehicles': [],\n",
    "        'training_times': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for e in range(episodes):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        total_waiting_time = 0\n",
    "        total_vehicles = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Take action and observe result\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store experience in memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            # Train agent on batch from memory\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        \n",
    "        # Calculate statistics for this episode\n",
    "        episode_time = time.time() - start_time\n",
    "        \n",
    "        # Save episode statistics\n",
    "        stats['episode_rewards'].append(total_reward)\n",
    "        stats['average_waiting_times'].append(total_waiting_time / max(1, step_count))\n",
    "        stats['average_vehicles'].append(total_vehicles / max(1, step_count))\n",
    "        stats['training_times'].append(episode_time)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Episode: {e+1}/{episodes}, Reward: {total_reward:.2f}, \"\n",
    "              f\"Epsilon: {agent.epsilon:.2f}, Time: {episode_time:.2f}s\")\n",
    "        \n",
    "        # Save agent every 10 episodes\n",
    "        if (e + 1) % 10 == 0:\n",
    "            agent.save(model_dir)\n",
    "            \n",
    "            # Save training statistics\n",
    "            with open(os.path.join(stats_dir, 'training_stats.json'), 'w') as f:\n",
    "                json.dump(stats, f)\n",
    "            \n",
    "            # Plot training progress\n",
    "            plot_training_progress(stats, stats_dir)\n",
    "    \n",
    "    # Save final agent\n",
    "    agent.save(model_dir)\n",
    "    \n",
    "    # Save final statistics\n",
    "    with open(os.path.join(stats_dir, 'training_stats.json'), 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "    \n",
    "    # Plot final training progress\n",
    "    plot_training_progress(stats, stats_dir)\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def plot_training_progress(stats, stats_dir):\n",
    "    \"\"\"\n",
    "    Plot training progress.\n",
    "    \n",
    "    Args:\n",
    "        stats: Dictionary of training statistics\n",
    "        stats_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    # Plot episode rewards\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(stats['episode_rewards'])\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    \n",
    "    # Plot average waiting times\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(stats['average_waiting_times'])\n",
    "    plt.title('Average Waiting Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Time')\n",
    "    \n",
    "    # Plot average vehicles\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(stats['average_vehicles'])\n",
    "    plt.title('Average Vehicles')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(stats_dir, 'training_progress.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1fe6967-b677-4d10-b407-c8bd926f6104",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m sumo_config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./sumo_config/config.sumocfg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# שנה זאת לנתיב הנכון\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# אימון הסוכן עם פחות אפיזודות לבדיקה\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m agent \u001b[38;5;241m=\u001b[39m train_agent(\n\u001b[0;32m      6\u001b[0m     sumo_config\u001b[38;5;241m=\u001b[39msumo_config_path,\n\u001b[0;32m      7\u001b[0m     episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# מספר קטן של אפיזודות לבדיקה\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      9\u001b[0m     model_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     stats_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(sumo_config, episodes, batch_size, model_dir, stats_dir)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTrain the RL agent.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    Trained agent\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Create directories if they don't exist\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_dir):\n\u001b[0;32m     17\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(model_dir)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(stats_dir):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# הגדרת נתיבים לקבצי קונפיגורציה\n",
    "sumo_config_path = \"./sumo_config/config.sumocfg\"  # שנה זאת לנתיב הנכון\n",
    "\n",
    "# נסיון אימון עם הסוכן המקורי\n",
    "try:\n",
    "    print(\"נסיון אימון עם הסוכן המקורי (DQNAgent)...\")\n",
    "    agent = train_agent(\n",
    "        sumo_config=sumo_config_path,\n",
    "        agent_class=DQNAgent,  # הסוכן המקורי עם lr\n",
    "        episodes=5,  # מספר קטן של אפיזודות לבדיקה\n",
    "        batch_size=32,\n",
    "        model_dir=\"models\",\n",
    "        stats_dir=\"stats\"\n",
    "    )\n",
    "    print(\"האימון הצליח עם הסוכן המקורי!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"שגיאה עם הסוכן המקורי: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nנסיון אימון עם הסוכן המתוקן (DQNAgentFixed)...\")\n",
    "        agent = train_agent(\n",
    "            sumo_config=sumo_config_path,\n",
    "            agent_class=DQNAgentFixed,  # הסוכן המתוקן עם learning_rate\n",
    "            episodes=5,\n",
    "            batch_size=32,\n",
    "            model_dir=\"models\",\n",
    "            stats_dir=\"stats\"\n",
    "        )\n",
    "        print(\"האימון הצליח עם הסוכן המתוקן!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"שגיאה עם הסוכן המתוקן: {e}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"\\nנסיון אימון עם הסוכן הפשוט (DQNAgentSimple)...\")\n",
    "            agent = train_agent(\n",
    "                sumo_config=sumo_config_path,\n",
    "                agent_class=DQNAgentSimple,  # הסוכן הפשוט ללא פרמטרים\n",
    "                episodes=5,\n",
    "                batch_size=32,\n",
    "                model_dir=\"models\",\n",
    "                stats_dir=\"stats\"\n",
    "            )\n",
    "            print(\"האימון הצליח עם הסוכן הפשוט!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"שגיאה עם הסוכן הפשוט: {e}\")\n",
    "            print(\"\\nכל הניסיונות נכשלו. בדוק את הודעות השגיאה ותקן בהתאם.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2be1b-cca6-4520-9111-74b4c9bf9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(sumo_config, agent_class=None, model_dir=\"models\", num_episodes=5, results_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Evaluate a trained agent.\n",
    "    \n",
    "    Args:\n",
    "        sumo_config: Path to SUMO configuration file\n",
    "        agent_class: Class of agent to use (if None, will try to determine from saved model)\n",
    "        model_dir: Directory containing trained models\n",
    "        num_episodes: Number of episodes to evaluate\n",
    "        results_dir: Directory to save evaluation results\n",
    "    \"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = TrafficSimEnv(sumo_config)\n",
    "    \n",
    "    # Determine which agent class to use\n",
    "    if agent_class is None:\n",
    "        # Try to find out which agent class was successful\n",
    "        try:\n",
    "            # First try the original agent\n",
    "            agent = DQNAgent(state_size=env.state_size, action_size=env.action_size)\n",
    "            agent.load(model_dir)\n",
    "            print(\"Using original DQNAgent for evaluation\")\n",
    "        except:\n",
    "            try:\n",
    "                # Then try the fixed agent\n",
    "                agent = DQNAgentFixed(state_size=env.state_size, action_size=env.action_size)\n",
    "                agent.load(model_dir)\n",
    "                print(\"Using DQNAgentFixed for evaluation\")\n",
    "            except:\n",
    "                # Finally try the simple agent\n",
    "                agent = DQNAgentSimple(state_size=env.state_size, action_size=env.action_size)\n",
    "                agent.load(model_dir)\n",
    "                print(\"Using DQNAgentSimple for evaluation\")\n",
    "    else:\n",
    "        # Use the specified agent class\n",
    "        agent = agent_class(state_size=env.state_size, action_size=env.action_size)\n",
    "        agent.load(model_dir)\n",
    "    \n",
    "    # Disable exploration for evaluation\n",
    "    agent.epsilon = 0\n",
    "    \n",
    "    # Statistics tracking\n",
    "    results = {\n",
    "        'episode_rewards': [],\n",
    "        'waiting_times': [],\n",
    "        'vehicle_counts': [],\n",
    "        'throughput': []\n",
    "    }\n",
    "    \n",
    "    # Evaluation loop\n",
    "    for e in range(num_episodes):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_waiting_times = []\n",
    "        step_vehicle_counts = []\n",
    "        vehicles_completed = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            # Select action (greedy policy)\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Track statistics\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Calculate waiting time and vehicle count for this step\n",
    "            waiting_time = 0\n",
    "            vehicle_count = 0\n",
    "            \n",
    "            for tl_id in env.traffic_lights:\n",
    "                incoming_lanes = env._get_incoming_lanes(tl_id)\n",
    "                \n",
    "                for lane in incoming_lanes:\n",
    "                    waiting_time += traci.lane.getWaitingTime(lane)\n",
    "                    vehicle_count += traci.lane.getLastStepHaltingNumber(lane)\n",
    "            \n",
    "            step_waiting_times.append(waiting_time)\n",
    "            step_vehicle_counts.append(vehicle_count)\n",
    "            \n",
    "            # Track vehicles that have completed their route\n",
    "            vehicles_completed = traci.simulation.getArrivedNumber()\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "        \n",
    "        # Save episode results\n",
    "        results['episode_rewards'].append(episode_reward)\n",
    "        results['waiting_times'].append(step_waiting_times)\n",
    "        results['vehicle_counts'].append(step_vehicle_counts)\n",
    "        results['throughput'].append(vehicles_completed)\n",
    "        \n",
    "        # Print progress\n",
    "        avg_waiting_time = np.mean(step_waiting_times) if step_waiting_times else 0\n",
    "        avg_vehicle_count = np.mean(step_vehicle_counts) if step_vehicle_counts else 0\n",
    "        \n",
    "        print(f\"Episode: {e+1}/{num_episodes}, Reward: {episode_reward:.2f}, \"\n",
    "              f\"Avg Waiting Time: {avg_waiting_time:.2f}, \"\n",
    "              f\"Avg Vehicles: {avg_vehicle_count:.2f}, \"\n",
    "              f\"Throughput: {vehicles_completed}\")\n",
    "    \n",
    "    # Save evaluation results\n",
    "    with open(os.path.join(results_dir, 'evaluation_results.json'), 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    \n",
    "    # Plot evaluation results\n",
    "    plot_evaluation_results(results, results_dir)\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_evaluation_results(results, results_dir):\n",
    "    \"\"\"\n",
    "    Plot evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of evaluation results\n",
    "        results_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    # Calculate summary statistics\n",
    "    avg_reward = np.mean(results['episode_rewards'])\n",
    "    avg_waiting_time = np.mean([np.mean(wt) for wt in results['waiting_times'] if wt])\n",
    "    avg_vehicle_count = np.mean([np.mean(vc) for vc in results['vehicle_counts'] if vc])\n",
    "    avg_throughput = np.mean(results['throughput'])\n",
    "    \n",
    "    # Plot episode rewards\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(range(len(results['episode_rewards'])), results['episode_rewards'])\n",
    "    plt.axhline(y=avg_reward, color='r', linestyle='-', label=f'Avg: {avg_reward:.2f}')\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot average waiting times\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i, wt in enumerate(results['waiting_times']):\n",
    "        if wt:  # Check if not empty\n",
    "            plt.plot(wt, alpha=0.3, label=f'Episode {i+1}' if i == 0 else \"\")\n",
    "    plt.axhline(y=avg_waiting_time, color='r', linestyle='-', label=f'Avg: {avg_waiting_time:.2f}')\n",
    "    plt.title('Waiting Times')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Time')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot vehicle counts\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i, vc in enumerate(results['vehicle_counts']):\n",
    "        if vc:  # Check if not empty\n",
    "            plt.plot(vc, alpha=0.3, label=f'Episode {i+1}' if i == 0 else \"\")\n",
    "    plt.axhline(y=avg_vehicle_count, color='r', linestyle='-', label=f'Avg: {avg_vehicle_count:.2f}')\n",
    "    plt.title('Vehicle Counts')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot throughput\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar(range(len(results['throughput'])), results['throughput'])\n",
    "    plt.axhline(y=avg_throughput, color='r', linestyle='-', label=f'Avg: {avg_throughput:.2f}')\n",
    "    plt.title('Throughput (Completed Vehicles)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'evaluation_results.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Summary text file\n",
    "    with open(os.path.join(results_dir, 'evaluation_summary.txt'), 'w') as f:\n",
    "        f.write(f\"Average Reward: {avg_reward:.2f}\\n\")\n",
    "        f.write(f\"Average Waiting Time: {avg_waiting_time:.2f}\\n\")\n",
    "        f.write(f\"Average Vehicle Count: {avg_vehicle_count:.2f}\\n\")\n",
    "        f.write(f\"Average Throughput: {avg_throughput:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dfbb40-bcf2-4aca-abe9-1c12d484a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# הרצת הערכה על המודל המאומן\n",
    "# שים לב: המערכת תנסה לזהות את סוג הסוכן המתאים באופן אוטומטי\n",
    "try:\n",
    "    evaluation_results = evaluate_agent(\n",
    "        sumo_config=sumo_config_path,\n",
    "        agent_class=None,  # נסה לזהות אוטומטית\n",
    "        model_dir=\"models\",\n",
    "        num_episodes=3,  # מספר קטן של אפיזודות להערכה\n",
    "        results_dir=\"results\"\n",
    "    )\n",
    "    print(\"ההערכה הושלמה בהצלחה!\")\n",
    "except Exception as e:\n",
    "    print(f\"שגיאה בהערכה: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
